{
  "hash": "75b0188868cb214362ff3dca88bfd3c3",
  "result": {
    "markdown": "---\ntitle: \"Wine & Feature Engineering\"\nimage: wine.png\ncategories: \n  - Machine Learning\n  - Data Analysis\n  - Data Science\n  - Feature Engineering\n  - Predictive Modeling\n---\n\n```{=html}\n<style>\nbody {\n  font-family: 'Times New Roman', sans-serif;\n  background-color: #f9f9f9;\n  color: #333;\n}\n\nh1, h2, h3, h4, h5, h6 {\n  color: #999;\n}\n\nnav {\n  background: url(\"https://www.shutterstock.com/shutterstock/videos/1069623523/preview/stock-footage-melbourne-australia-mar-motorized-moving-shot-of-new-albums-released-in-spotify-app.mp4\") no-repeat top center fixed;\n  background-size: cover;\n  height: 20vh;\n  width: 100%;\n}\n\n.panel-tabset {\n  border: 1px solid #ddd;\n  border-radius: 5px;\n  background-color: #fff;\n  padding: 15px;\n  box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n}\n\n.panel-tabset h3 {\n  margin-top: 0;\n}\n\n.panel-tabset .tab-content {\n  padding: 10px 0;\n}\n</style>\n```\n\n## Overview\n\nIn this analysis, we will generate a comprehensive set of 10 features derived from the wine dataset, which includes the points feature. Through the process of feature engineering, we will select and transform variables to create meaningful features that enhance the predictive power of our model. This involves consolidating similar categories using functions like fct_lump and ensuring that our dataset is clean by removing any rows with missing values. Additionally, we will transform the price variable into its logarithmic form (log(price)) to stabilize the variance and normalize the distribution, facilitating a more effective linear regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(fastDummies)\nwine = read_rds(\"/Users/Shared/Data 505/wine.rds\")\n```\n:::\n\n\n## Feature Engineering\n\n**Summary**\n\nIn this section, we will create a total of 10 features, including the points feature, from the wine dataset. We will also remove all rows that contain any missing values to ensure the data is clean and complete. Finally, we will transform the price into its logarithmic form and ensure that only the log-transformed price (log(price)) and the selected features remain in the final dataframe, which we will call wino.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwino <- wine %>%\n  mutate(lprice = log(price)) %>%\n  mutate(country = fct_lump(country, 5),\n         taster_name = fct_lump(taster_name, 5),\n         variety = fct_lump(variety, 5),\n         winery = fct_lump(winery, 5),\n         region_1 = fct_lump(region_1, 5),\n         province = fct_lump(province, 5),\n         designation = fct_lump(designation, 5)) %>%\n  select(lprice, points, country, taster_name, variety, winery, region_1, province, designation) %>%\n  drop_na()\n```\n:::\n\n\n## Model Training with Caret\n\n**Summary**\n\nThis section focuses on using the Caret library to partition the wino dataframe into an 80% training set and a 20% test set. We will perform a linear regression with bootstrap resampling and report the Root Mean Squared Error (RMSE) for the model on the test set. The bootstrap method involves resampling with replacement, which helps in estimating the accuracy of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(504)\nwine_index <- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\nwino_tr <- wino[wine_index, ]\nwino_te <- wino[-wine_index, ]\n\ncontrol <- trainControl(method = \"boot\", number = 5)\nm1 <- train(lprice ~ ., \n            data = wino_tr, \n            method = \"lm\",\n            trControl = control)\n\nprint(m1$resample)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       RMSE  Rsquared       MAE  Resample\n1 0.4716720 0.4639796 0.3669938 Resample1\n2 0.4735139 0.4531511 0.3691902 Resample2\n3 0.4682085 0.4532082 0.3652813 Resample3\n4 0.4724564 0.4578321 0.3690610 Resample4\n5 0.4718331 0.4647390 0.3683953 Resample5\n```\n:::\n\n```{.r .cell-code}\nwine_pred <- predict(m1, wino_te)\npostResample(pred = wine_pred, obs = wino_te$lprice)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     RMSE  Rsquared       MAE \n0.4716279 0.4453607 0.3677621 \n```\n:::\n:::\n\n\n## Variable Selection\n\n**Summary**\n\nIn this section, we will identify and visualize the importance of the selected features in our model. The goal is to understand which features have the most significant impact on predicting the log-transformed price of the wine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance <- varImp(m1, scale = TRUE)\nplot(importance)\n```\n\n::: {.cell-output-display}\n![](Wine_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Data Partition\n\n**Summary**\n\nTo ensure reproducibility, we will set the seed to 504 before partitioning the data into training and test sets. We will aim to achieve an RMSE on the test data of less than 0.47 for 1 point, less than 0.46 for 2 points, or less than 0.45 for 3 points. This ensures that the model's performance is both robust and reproducible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(504)\nwine_pred <- predict(m1, wino_te)\npostResample(pred = wine_pred, obs = wino_te$lprice)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     RMSE  Rsquared       MAE \n0.4716279 0.4453607 0.3677621 \n```\n:::\n:::\n\n\n## Conclusion:\n\nThis presentation demonstrated the process of feature engineering, model training using the Caret library, and evaluating the model's performance. Key steps included creating features, handling missing data, partitioning the dataset, and assessing the model using RMSE. The importance of reproducibility in data partitioning and model evaluation was emphasized through the use of set.seed.\n",
    "supporting": [
      "Wine_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}